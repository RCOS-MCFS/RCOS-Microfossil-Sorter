<html>
<head>
    <title>RCOS Microfossil Sorter</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link href="static/style.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
</head>

<body>
<div class="jumbotron">
    <div class="backdrop">
    </div>
    <h1 class="display-3">Microfossil Concentrate Sorter</h1>
    <p class="lead">Olivia Lundelius and John Klingelhofer - RCOS Spring 2018</p>
    <a href="https://github.com/RCOS-MCFS/RCOS-Microfossil-Sorter" class="btn">View on GitHub</a>
    <a href="index.html" class="btn">Back to Main Page</a>
</div>

<div class="stretch">
    <div class="container">
<div class="overview">
    <font color="white"><h2>Overview</h2>
    In paleontology, microfossil concentrate is a mixture of microfossils (typically from rodents and lizards) and gravel, with each particle being about 2 mm in size. Microfossils can potentially serve as a source of big data for paleontologists about the locations and environments from which they were collected. In order to study the microfossils, they must be sorted out from the gravel, which is a very time consuming process. This project aims to build a machine that automatically sorts microfossils from concentrate, and for that machine's assembly and software to be made open source and accessible by paleontology laboratories. </font>
</div>
    </div>
</div>
    
    
    
    


<div class="container">
    
    
    
    
    
    
    
    <div class="blog-entry">
<h2 class="blog-title"> Week 6? </h2>
<h4 class="blog-date">November 4, 2018</h4>
<p class="blog-body">
    I made a hopper out of posterboard which has a nose for evenly distributing the concentrate.
A diagram of what it is supposed to like is attached to this post (or will be, when I figure out how to do that).
The dimensions are 0.5 x 0.5 cm square bottom opening, 5.5 x 5.5 cm square top opening, and the diagonal distance from center 
    of top to center of bottom of a side of the hopper is 6 cm.
Liz's cardboard conveyor belt is coming along very well, and Alexis has added some nice layman's documentation and is working 
    on more in-depth documentation of how the program works for easier modification.
    Also, I realized that a lab shaker/oscillator should be perfect for a vibratory bowl feeder. I'm convinced now that this is the
    way to go. I believe that this is what Tom Kaye used in his paper.
</p><p>
    I decided to scrap a lot of the machine learning approaches I was doing before and start working through a tutorial on 
    OCR/handwriting recognition. 
    I think I need to understand more about something called HOGs, histogram of oriented gradients? 
    Hopefully shouldn't be too bad.  
 </p>
 <p class="signature"> - Olivia</p>
</div>
<hr />
    
    
    
    
    
    
    
    
    
    
    
    <div class="blog-entry">
<h2 class="blog-title"> Week 5 </h2>
<h4 class="blog-date">October 26, 2018</h4>
<p class="blog-body">
    Regarding hardware, I started looking at vibratory bowl feeders again.  Although they are more complicated to print and assemble,
    they might be more effective than just a hopper.
    I found a couple of models online, however I can't find a license attached to them, so they can't be assumed open source.
    I'll look into maybe making a similar (but much simpler) model myself.
    I'm worried about sediment getting either damaged or stuck in the hopper.  However, I'm still not sure how to get it to vibrate.
    For a hopper, it may just be simpler to make one out of posterboard before trying to print one.
 </p>
 <p class="signature"> - Olivia</p>
</div>
<hr />
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="blog-entry">
<h2 class="blog-title"> Week 4? 5? </h2>
<h4 class="blog-date">October 21, 2018</h4>
<p class="blog-body">
    I've been looking into how to get the shape classifier working again.  This is possibly one of the most important parts of
    the classifier because if it works okay, then it would be applicable to a much wider range of data sets.
    There's Hu Moments, contours, edges, and image segmentation.  I need to figure out what I want to use and how to train a
    model to recognize the difference between microfossils and gravel based on its contours.
</p><p>
    I'm not seeing a lot of information on training using contours, which I find a bit odd.  Maybe I'm approaching this incorrectly?
    Once I figure out what method I can use, I can just add a class to approach.py to test how well contouring works individually.
    If it ends up working well, I may move it to a separate file to see if I can use it in conjunction with color segmentation as well.
    I've been reading <a href="https://mmeysenburg.github.io/image-processing/09-contours/">this</a>, <a href="https://www.pyimagesearch.com/2014/10/27/opencv-shape-descriptor-hu-moments-example/">this</a>, and <a href="http://www.robots.ox.ac.uk/~jmb/lectures/InformaticsLecture5.pdf">this</a>.
    I may be able to re-appropriate parts of the data collection script for the shape discriminator.
 </p>
 <p class="signature"> - Olivia</p>
</div>
<hr />
    
    
    
    
    
    
    <div class="blog-entry">
<h2 class="blog-title"> F18 Semester start </h2>
<h4 class="blog-date">October 16, 2018</h4>
<p class="blog-body">
    This semester (F18), three students are working on this project: myself, Alexis Joseph and Elizabeth Chavarin.
</p><p>
    So far, previous ideas have been:
    Rotary sorter, which is being abandoned due to over-complication of project.
    Some kind of copy of <a href="https://www.tomkaye.com/2-uncategorised/32-fully-automated-micro-fossil-picker">Kaye's sorter</a>, which is being abandoned due to difficulty of acquiring/making a vibratory bowl feeder
    (not sure how to make it vibrate).
 </p><p>
    Now, current idea is to have a simple hopper deposit some small amount of concentrate onto a conveyor belt.  The concentrate
    would be photographed and an air puff would remove the sample from the assembly line if microfossils are detected.  Everything
    else is dumped into a waste bin.
    The removed microfossils will likely have gravel still mixed in with it, so there should probably be a system for re-sorting
    material multiple times in order to better isolate microfossils from gravel.
    We will probably need to make use of 3D printed pieces, some PVC, plastic sheets, and some kind of motor.  We'll need to figure
    out the gear ratios possibly, which will be annoying.
    I found a model conveyor belt tutorial <a href="https://www.youtube.com/watch?v=EOPCZhKGPLA">here</a>.
 </p>
 <p class="signature"> - Olivia</p>
</div>
<hr />
    
    
    
    
    <div class="blog-entry">
<h2 class="blog-title"> Week 4 </h2>
<h4 class="blog-date">May 2, 2018</h4>
<p class="blog-body">
    Managed to get nanpy partially working.  After messing around with different versions, I got python code to work with LEDs
    but for some reason I can't manage to control the servo motors.  I'd much rather use an Arduino to control the various
    parts of this project as it'll allow for maybe lighting condition optimization in the future, but for now I may have to
    go back to using a servo HAT to control a servo as that'll be much easier in the long run.
</p><p>
    I've also acquired and tested out a pi camera, seems to work fine.  I'm currently working on, and over the summer, will be
    working on getting the pi camera to work with existing code, and am also making a script to identify when an object is around
    the center of the camera.  That would be like a "sample" event, at which point a discrimination algorithm will determine
    the classification of the centered object and send an appropirate signal to the servo (hopefully through nanpy, but if I
    have to I guess I'll switch to the servo HAT).
 </p><p>
    I've made a small chute out of black poster board with the raspberry pi, camera, LEDs and breadboard as a kind of prototype,
    but I'm fairly certain I'll need the help of a mech-e to really get the hardware aspects of this project going.  I'm going
    to ask around to see if someone knows paleontologists or engineers who may be able to help me with this.  I plan on working
    through the summer, when I have much more time and am not slowly suffering through classes, on getting together a more solid
    demo and hopefully a much less flimsy hardware prototype.  Hopefully, I can prepare everything in time for the Association
    of Materials and Methods in Paleontology meeting next year, it would be an excellent platform to gather support for this
    project.  When I attended last year, presenting the idea for this project, it gained some attention, so hopefully I'll
    have a more fleshed-out project to show them.
 </p>
 <p class="signature"> - Olivia</p>
</div>
<hr />
    
    
    
    
<div class="blog-entry">
    <h2 class="blog-title"> Week 9 </h2>
    <h4 class="blog-date">April 6, 2018</h4>
    <p class="blog-body">
        After being sick and useless for a week I've started looking more into the hardware for the sorter again.  It
        occurs to me that we'll need some sort of extra cables or something for the LEDs and components to be more movable
        and so that they can be placed more easily in some kind of housing for the sorter electronics, rather than being
        stuck to the breadboard.  Going to have to look into getting some of those.
    </p><p>
        It's been suggested that we look into using an air puff rather than a lever conrolled by a servo motor.  That's
        definitely a good idea, however it will probably generally be easier for assembly if we could get the lever to work.
        It would also reduce the cost of having some kind of pressurized air puff as well as figuring out how to control it,
        wheras servo motor works nicely with the Elegoo/Arduino.  
    </p><p>
        I've started teaching myself to use NX 11 modeling software.  Definitely nowhere near needing to use it yet, first
        prototypes which I'm working on are going to be made of cardboard.  Only when I have something that kind of works with
        that am I going to try making anything with that modeling software.  I guess I could just model everything in Unity but
        that feels kind of unprofessional and it probably won't be in the right kind of file format for 3D printing.  Also
        unnecessarily bloat-y software for just making a model. Anyways, cardboard is proving to be fairly difficult to work with
        considering how small microfossils are.  Also have a mild concern about static getting them caught up, especially if
        these models end up being printed from plastic, which are prone to static.  Tom Kaye's model was made from some kind of
        metal, which likely prevented this from being an issue.
    </p><p>
        At some point I must acquire a webcam so that I can try and get the lighting of this correct.  Placement of LEDs will be
        difficult as they'll have to be close enough to the sample to illuminate it without shadows (likely one on either side)
        but not so that it causes any kind of over-saturation from the camera's perspective or glare, which would throw the whole
        thing off.  This will likely affect size and shape of the container for image capture and possibly the chute that
        microfossils/gravel travel down.  Still not sure how to get the timing and placement of samples right.  Maybe they can be
        slowly sliding down a chute and use edge detection to take a picture when it's within a certain area of the frame, like
        the center.  That'll probably work.
    </p><p>
        Another thing that could be done is have some kind of frosted side paneling to the area where a picture will be taken,
        and have the light be filtered through that.  That way it'll soften any glare, hopefully, provide smoother and more consistent
        lighting and allow for a much smaller chute area, and reduce the need for careful placement of LEDs.  They could then just
        be shoved on either side of the sample wherever is most convenient.
    </p><p>
        Also vaguely related, we might submit our project to the Undergraduate Research Symposium.  Or not.  Either way, if not
        this year, will probably do so next year as it could be an opportunity to find people able to help on the mechanical sorter
        as I'm not sure what I'm doing and currently just trying my best to get this to hopefully work.
    </p><p>
        Anyways, tl;dr starting to make cardboard models of the physical sorter, need to get a webcam and more wires and some kind
        of frosted surface as a light filter (probably plastic wrap would be fine for now), learning how to use NX 11.
    </p>
    <p class="signature"> - Olivia</p>
</div>
<hr />

    <div class="blog-entry">
        <h2 class="blog-title"> Week 6, 7, 8, 9 </h2>
        <h4 class="blog-date">April 3rd, 2018</h4>
        <p class="blog-body">
            <h4> Long time, no see.</h4>
            Phew! Long time since our last blog post, which was done before spring break. But, this silence was not an
            indicator of a lack of activity, but rather a symbol of our focus on our work! These few weeks have been
            spent with leaning much more heavily into the visual component for the purpose of our demo which
            was demonstrated in our class presentation last week, and working on a version of our project better suited
            for laboratory settings.
            <br><br>
            <h4> What we've done. </h4>
            Before we get to the contents of our demo, it's worth touching on the advancements in our technology that
            have been made. As I started to lean into the live webcam based utility, it rapidly became apparent that
            the algorithm I'd made for a quick and dirty image cropping was entirely too slow for use with a webcam,
            as it slowed the display of the webcam's feed to about one frame a second. Much to my delight, and
            partial annoyance that I hadn't checked for this option before leaning so heavily into my own approach,
            it's relatively easy to string together a few OpenCV functions to detect contours in images, as well as
            some built in functions for checking the areas of these coordinates, and other such things. And most
            importantly, this function blew mine away in terms of speed.
            <br><br>
            With functions for these contours added to the img_tools library, they were applied in two settings:
            gathering data, and live labeling of data. The data gathering program is relatively simple, you
            run it, it shows a feed of webcam, and highlights the largest contour in the image. If the lighting environment
            is set up correctly, this should be the only, or largest, bone/rock in the frame. Then, the user presses
            'r' for 'b' to save the contents of that highlighted contour to a folder for rocks and bones respectively.
            <br><br>
            With the data gathered, we run the live data labeler. Running this file, currently live_demo.py though
            it will be soon be refactored into a more elegant form, the user specifies the type of model they wish to
            use, as well as the folders for the bone and rock data being used to train it. The specified model will then
            convert these images into the appropriate format, and train on a portion of this data and print accuracy, to
            give the user a baseline of how accurate it will be on live data. Then, it shows the webcam feed, labeling
            rocks and bones as needed.
            <br><br>
            <h4> What's next? </h4>
            Aside from the hardware, which Olivia will probably post an update on in the next week or so, upcoming software
            tasks will be:<ul>
                <li>
                    Escape the command line! Since we want this to be as usable as possible, we'll work on making
                    a more traditional UI for selecting all of what are currently command line options.
                </li>
                <li>
                    Refactor our demo, which is currently a  long script, into a series of methods, for easier use and
                    call by the aforementioned new UI, and easier extensibility in the future.
                </li>
                <li>
                    Set up some utility for changing the image thresholding depending on lighting
                    conditions, as our in-class demo classifier was ineffective because the lighting differed so much
                    from the environment in which the currently hard-coded parameters for the threshold were established.
                    Unfortunately, the typical approach of using a single object as a baseline with a saved perfect threshold
                    for that object isn't entirely feasible in our case, since the microfossils are so incredibly small
                    I can't fathom what common object could be used as this baseline, especially since no 3d printer could print
                    something to that side. We could go with an "eye-doctor" type approach for establishing the threshold, where
                    it cycles through various parameters asking the user if it looks better or worse before settling on a final option.
                    Food for thought!
                </li>
                <li>
                    Model improvement. I, because it was fun more than it was practical, wrote some of the models used
                    from their equations, and should probably transfer these methods over to applications of these models
                    which exist in certain ML libraries which are likely to be faster.
                </li>
                <li>
                    Additional data gathering. With binary classification pretty much done, we should start examining
                    additional details that can be automatically gathered.
                </li>
            </ul>

        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 5 </h2>
        <h4 class="blog-date">March 6, 2018</h4>
        <p class="blog-body">
            Recently got a micro servo motor (SG51R).  Seems fairly slow, but that shouldn't be too much of a problem, hopefully.
        </p><p>
            I've decided to greatly simplify the design for the sorter to more closely resemble the one created by Tom Kaye.  In this case,
            we'll need only one servo (hopefully the one we have should suffice) and will only need minimal assembly and designing.  This
            way, all we'll have to make is a hopper, a path for a sample to go on, and determine the distance needed for the sample to go
            in order to give the algorithm and servo enough time to determine where to put the sample.
        </p><p>
            UV LEDs arrived, we'll be able to assemble a lot more now.  Hardware-wise, the only things left are the hopper, path, and some
            kind of material for photographing the samples against.  This break, I (Olivia) will be able to set up my R-Pi with the servo
            HAT and a dummy LED, and hopefully put together a script that incorporates both the discrimination algorithm and controlling the
            servo.  John has a small web cam that works with the R-Pi, which should be useful.  So far as designing hardware, I'll try to
            make something over break but I'm not sure how successful I'll be.  Probably I'll just go to the Forge again and see if they can
            help, don't want to have to print a whole bunch of garbage before finally getting it right.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 4 </h2>
        <h4 class="blog-date">February 27, 2018</h4>
        <p class="blog-body">
            Went to RPI Forge, which was extremely helpful.  Forge is mostly for 3D printing, the Embedded Hardware Club would 
            likely be more helpful for what we're doing, but someone at Forge helped me.  Apparently an Arduino isn't actually all
            that necessary, and the same things can be controlled by various modules that can connect to a Raspberry Pi directly rather
            than having the extra complication of going through an Arduino and trying to sync up a camera module through R-Pi and motor
            modules through Arduino.
        </p><p>
            It was suggested that we 3D print a hopper through Forge, I've been offered help with that by an individual named Misha.
            We should probably have two kinds of servos, a continuous one for the central rotating mechanism, and a positional rotation servo
            for a chute to deposit samples into one bin or another.
        </p><p>
            So far as getting everything to be used directly through the R-Pi, there's a small hardware piece that can be used directly
            through the R-Pi rather than be forced to use the Arduino interface.  There's a Python package (something like python-smbus)
            that works for this servo HAT after enabling I^2C on the R-Pi.  Not sure how to do any of that yet, will be working this week
            to get that working, and also to get two servos and the servo HAT.  It was suggested that we laser-cut the device out of
            polystyrene, but that's something further off into the future.  It was also suggested that we get a small spectrometer module,
            which, according to Misha, works very well.  Should try using that either instead of or in addition to the camera module.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 4 </h2>
        <h4 class="blog-date">February 26, 2018</h4>
        <p class="blog-body">
            SVM classifier was fixed, now looking further into gathering hardware components. Ordered an Elegoo board on Amazon,
            as well as some wires, a breadboard and other components to start off.  This week we'll look at the performance
            of the classification algorithms on the Raspberry Pi (which should be just fine) and start using the Elegoo (which is
            exactly the same as an Arduino, just under a different name).
        </p><p>
            Am going to go to RPI Foundry to see what kind of help they can give us on our project, any hardware suggestions and
            maybe even find a Mech-E who's willing to join and help us.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 3 </h2>
        <h4 class="blog-date">February 15, 2018</h4>
        <p class="blog-body">
            This week's is among the final weeks for the classifier, unless later in the semester we move beyond binary
            classification. There's now a function in place which can take an image containing multiple bones/rocks
            or a single bone/rock and crop an image containing just that image, provided that the image is taken on a
            clear enough background such that edge detection can form a general outline of the rock/stone in question.
        </p><p>
            This cropping may possibly prove useful as an additional metric in classification, as the rocks tend to have
            a much more uniform aspect ratio (often about 1:1) when compared to the bones. In addition, this automatic
            cropping makes obtaining measurements for the samples easy, which itself could prove an interesting
            additional metric. Though the clear difference in color between the rocks and bones under UV light may
            make these metrics superfluous, they may still have some purpose or have a role in broader analysis of
            collected samples.
        </p><p>
            This week also saw more work on different classifiers. Since differences in average color should be enough
            to segment the data, we developed a simple perceptron, and began work on a SVM. We also changed the structure
            of functions within files, with all of the image manipulations being moved into the "img_tools.py" file which
            can be imported into any function, rather than each classifier having its own subset of a broader collection
            of functions.
        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 2 </h2>
        <h4 class="blog-date">February 8, 2018</h4>
        <p class="blog-body">
            This week's work has largely been focused on the Computer Vision aspect of this project. Of course an important part of this process was the creation of new training data under more controlled conditions, as well as examining the various modes available for the classification of this data.
        </p><p>
            Increasing the expected ease of this process, one of the major delimitating factors between bones and stone is the average color. Provided that this classiciation difference holds true for all of our upcoming samples, aside from those which we have currently, each image can be reduced to a simple vector containing the average R, G, and B values. With the normalcy of this data (not the case were we interpreting the images directly), we can apply standard data mining techniques to this data such as SVMs, Linear Regression, etc. Our old training data could be inearly separated based on these RGB values, and we expect this to hold true for the upcoming data.
        </p><p>
            This upcoming week will include nailing down our classification strategy for future weeks, as well as starting to make some headway and decisions for the hardware aspect of this project.
        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 1 </h2>
        <h4 class="blog-date">February 1, 2018</h4>
        <p class="blog-body">
            This first week was largely focused on laying the foundation for this project, as a strong early foundation means time saved later on. This foundation week included the setup of our GitHub and this website, installing all of the packages to be used in this assignment on the comptuers of both team members (including the often persnickity OpenCV) and discussing some of the broader goals for the project.
        </p>
        <p class="signature"> - John</p>
    </div>
</div>
</body>
</html>
